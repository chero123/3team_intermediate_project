# ============================================
# ì„ë² ë”© ëª¨ë¸ & ì²­í‚¹ ì „ëµ ë¹„êµ í‰ê°€ ì‹œìŠ¤í…œ
# 15ê°€ì§€ ì¡°í•© í…ŒìŠ¤íŠ¸ (ì²­í‚¹ 5ê°€ì§€ Ã— ì„ë² ë”© 3ê°€ì§€)
# ============================================

import os
import json
import time
import numpy as np
import dotenv
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
from datetime import datetime


# ============================================
# ë°ì´í„° í´ë˜ìŠ¤
# ============================================


@dataclass
class EvaluationResult:
    """í‰ê°€ ê²°ê³¼"""

    chunking_method: str
    embedding_model: str
    hit_rate_at_1: float
    hit_rate_at_3: float
    hit_rate_at_5: float
    mrr: float  # Mean Reciprocal Rank
    avg_latency_ms: float
    total_chunks: int
    embedding_dim: int
    # ë¶€ë¶„ì ìˆ˜ (Answer í’ˆì§ˆ)
    keyword_precision: float = 0.0  # must_include í‚¤ì›Œë“œ í¬í•¨ë¥ 
    keyword_recall: float = 0.0  # relevant_keywords ë§¤ì¹­ë¥ 
    answer_quality_score: float = 0.0  # ì¢…í•© ì ìˆ˜
    total_time_seconds: float = 0.0  # ì¶”ê°€: í…ŒìŠ¤íŠ¸ ì´ ì†Œìš”ì‹œê°„


# ============================================
# ì„ë² ë”© ëª¨ë¸ ë˜í¼
# ============================================


class EmbeddingModelWrapper:
    """ì„ë² ë”© ëª¨ë¸ í†µí•© ë˜í¼"""

    def __init__(self, use_gpu: bool = True):
        self.models = {}
        self.openai_client = None
        self.device = "cuda" if use_gpu else "cpu"

        # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ ì²´í¬
        try:
            import torch

            if use_gpu and torch.cuda.is_available():
                self.device = "cuda"
                print(f"[GPU] CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name(0)}")
            else:
                self.device = "cpu"
                if use_gpu:
                    print("[GPU] CUDA ì‚¬ìš© ë¶ˆê°€, CPU ì‚¬ìš©")
        except ImportError:
            self.device = "cpu"
            print("[GPU] PyTorch ì—†ìŒ, CPU ì‚¬ìš©")

    def load_model(self, model_name: str):
        """ëª¨ë¸ ë¡œë“œ (lazy loading)"""
        if model_name in self.models:
            return

        if model_name == "openai":
            try:
                from openai import OpenAI

                self.openai_client = OpenAI()
                self.models[model_name] = "openai"
                print(f"  [âœ“] OpenAI text-embedding-3-small ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                print(f"  [âœ—] OpenAI ë¡œë“œ ì‹¤íŒ¨: {e}")
                raise
        else:
            try:
                from sentence_transformers import SentenceTransformer

                model_map = {
                    "BGE-m3-ko": "dragonkue/BGE-m3-ko",
                    "MiniLM": "all-MiniLM-L6-v2",
                    "ko-sroberta": "jhgan/ko-sroberta-multitask",
                }
                if model_name not in model_map:
                    raise ValueError(f"Unknown model: {model_name}")

                print(f"  [ë¡œë”©] {model_name} ëª¨ë¸ ë¡œë”© ì¤‘... (device: {self.device})")
                self.models[model_name] = SentenceTransformer(
                    model_map[model_name], device=self.device
                )
                print(f"  [âœ“] {model_name} ë¡œë“œ ì™„ë£Œ ({self.device})")
            except Exception as e:
                print(f"  [âœ—] {model_name} ë¡œë“œ ì‹¤íŒ¨: {e}")
                raise

    def encode(self, texts: List[str], model_name: str) -> np.ndarray:
        """í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜"""
        self.load_model(model_name)

        if model_name == "openai":
            embeddings = []
            # ë°°ì¹˜ ì²˜ë¦¬ (OpenAIëŠ” í•œ ë²ˆì— ìµœëŒ€ 2048ê°œ)
            batch_size = 100
            for i in range(0, len(texts), batch_size):
                batch = texts[i : i + batch_size]
                response = self.openai_client.embeddings.create(
                    model="text-embedding-3-small", input=batch
                )
                for item in response.data:
                    embeddings.append(item.embedding)
            return np.array(embeddings)
        else:
            model = self.models[model_name]
            # ì²­í¬ê°€ ë§ìœ¼ë©´ ì§„í–‰ë¥  í‘œì‹œ
            show_progress = len(texts) > 100
            return model.encode(texts, show_progress_bar=show_progress)

    def get_embedding_dim(self, model_name: str) -> int:
        """ì„ë² ë”© ì°¨ì› ë°˜í™˜"""
        dim_map = {
            "BGE-m3-ko": 1024,
            "MiniLM": 384,
            "ko-sroberta": 768,
            "openai": 1536,
        }
        return dim_map.get(model_name, 0)


# ============================================
# í‰ê°€ ì‹œìŠ¤í…œ
# ============================================


class RAGEvaluator:
    """RAG ì‹œìŠ¤í…œ í‰ê°€ê¸°"""

    def __init__(self, base_data_dir: str = "data"):
        self.base_data_dir = base_data_dir
        self.embedding_wrapper = EmbeddingModelWrapper()

        # ì²­í‚¹ ë°©ì‹ ì •ì˜
        self.chunking_methods = {
            "chunking_data1": "ì•ˆíŒ€ì›-Recursive",
            "chunking_data2": "ë°•íŒ€ì›-Paragraph",
            "chunking_data3": "ì„œíŒ€ì›-Semantic",
            "chunking_data4": "ê¹€íŒ€ì›-ContextEnriched",
            "chunking_data5": "ì¥íŒ€ì›-Hierarchical",
        }

        # ì„ë² ë”© ëª¨ë¸ ì •ì˜
        self.embedding_models = [
            "BGE-m3-ko",
            "MiniLM",
            "ko-sroberta",
            "openai",
        ]

    def load_chunks(self, chunking_folder: str) -> List[Dict]:
        """ì²­í‚¹ëœ ë°ì´í„° ë¡œë“œ"""
        folder_path = os.path.join(self.base_data_dir, chunking_folder)
        all_chunks = []

        if not os.path.exists(folder_path):
            print(f"  [ê²½ê³ ] í´ë” ì—†ìŒ: {folder_path}")
            return all_chunks

        json_files = [f for f in os.listdir(folder_path) if f.endswith("_chunked.json")]

        for json_file in json_files:
            file_path = os.path.join(folder_path, json_file)
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    for chunk in data.get("chunks", []):
                        chunk["source_doc"] = data.get("doc_id", "")
                        all_chunks.append(chunk)
            except Exception as e:
                print(f"  [ì˜¤ë¥˜] {json_file} ë¡œë“œ ì‹¤íŒ¨: {e}")

        return all_chunks

    def load_evaluation_dataset(self, eval_file: str) -> List[Dict]:
        """í‰ê°€ ë°ì´í„°ì…‹ ë¡œë“œ"""
        with open(eval_file, "r", encoding="utf-8") as f:
            data = json.load(f)
        return data.get("evaluation_set", [])

    def cosine_similarity(self, a: np.ndarray, b: np.ndarray) -> float:
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-9)

    def find_relevant_chunks(
        self,
        chunks: List[Dict],
        answer: str,
        keywords: List[str],
        expected_chunk_ids: List[str] = None,
    ) -> List[str]:
        """
        ì •ë‹µ ì²­í¬ ID ì°¾ê¸°
        1ìˆœìœ„: expected_chunk_ids (Ground Truthì—ì„œ ì§€ì •í•œ ì²­í¬)
        2ìˆœìœ„: answer í…ìŠ¤íŠ¸ í¬í•¨ ì—¬ë¶€
        3ìˆœìœ„: í‚¤ì›Œë“œ 2ê°œ ì´ìƒ ë§¤ì¹­
        """
        relevant_ids = []

        # 1ìˆœìœ„: expected_chunk_idsê°€ ìˆìœ¼ë©´ í•´ë‹¹ íŒ¨í„´ê³¼ ë§¤ì¹­ë˜ëŠ” ì²­í¬ ì°¾ê¸°
        if expected_chunk_ids:
            for chunk in chunks:
                # ì²­í¬ì˜ document IDì™€ ë‚´ë¶€ chunk IDë¥¼ ê²°í•©í•˜ì—¬ ì „ì²´ ID ìƒì„±
                full_chunk_id = (
                    f"{chunk.get('source_doc', '')}::{chunk.get('id', '')}"
                    if chunk.get("source_doc")
                    else chunk.get("id", "")
                )

                for expected_id in expected_chunk_ids:
                    # expected_idê°€ 'doc_id::chunk_id' í˜•ì‹ì¼ ê²½ìš°,
                    # chunkì˜ full_chunk_idì™€ ì§ì ‘ ë¹„êµí•˜ì—¬ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
                    if full_chunk_id == expected_id:
                        relevant_ids.append(chunk["id"])
                        break

            # 1ìˆœìœ„ì—ì„œ ì°¾ì§€ ëª»í–ˆê³ , ê·¸ë˜ë„ expected_chunk_idsê°€ ìˆë‹¤ë©´,
            # ì˜ˆì™¸ì ìœ¼ë¡œ doc_id ë¶€ë¶„ë§Œìœ¼ë¡œ ë§¤ì¹­ ì‹œë„ (ì´ëŠ” ëœ ì •í™•í•  ìˆ˜ ìˆìŒ)
            if not relevant_ids and expected_chunk_ids:
                for chunk in chunks:
                    chunk_source_doc = chunk.get("source_doc", "")
                    for expected_id in expected_chunk_ids:
                        expected_doc_part = (
                            expected_id.split("::")[0]
                            if "::" in expected_id
                            else expected_id
                        )
                        if (
                            expected_doc_part
                            and expected_doc_part.lower() in chunk_source_doc.lower()
                        ):
                            # ì¤‘ë³µ ì¶”ê°€ ë°©ì§€ë¥¼ ìœ„í•´ ì´ë¯¸ ì¶”ê°€ëœ chunk_idëŠ” ê±´ë„ˆëœ€
                            if chunk["id"] not in relevant_ids:
                                relevant_ids.append(chunk["id"])
                            break

        # 2ìˆœìœ„: answer í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ ì²­í¬
        if not relevant_ids and answer:  # 1ìˆœìœ„ì—ì„œ ì°¾ì§€ ëª»í–ˆì„ ë•Œë§Œ 2ìˆœìœ„ ì‹œë„
            for chunk in chunks:
                text = chunk.get("text", "").lower()
                # answerì˜ í•µì‹¬ ë¶€ë¶„ë§Œ ì²´í¬ (ì²« 50ì)
                answer_core = (
                    answer[:50].lower() if len(answer) > 50 else answer.lower()
                )
                if answer_core in text:
                    relevant_ids.append(chunk["id"])

        # 3ìˆœìœ„: í‚¤ì›Œë“œ ë§¤ì¹­ (ìµœì†Œ 2ê°œ ì´ìƒ)
        if not relevant_ids:  # 1,2ìˆœìœ„ì—ì„œ ì°¾ì§€ ëª»í–ˆì„ ë•Œë§Œ 3ìˆœìœ„ ì‹œë„
            for chunk in chunks:
                text = chunk.get("text", "").lower()
                keyword_matches = sum(1 for kw in keywords if kw.lower() in text)
                if keyword_matches >= 2:
                    relevant_ids.append(chunk["id"])

        return relevant_ids

    def calculate_answer_quality(
        self,
        retrieved_text: str,
        must_include: List[str],
        relevant_keywords: List[str],
    ) -> Tuple[float, float, float]:
        """
        ë‹µë³€ í’ˆì§ˆ ë¶€ë¶„ì ìˆ˜ ê³„ì‚°
        Returns: (keyword_precision, keyword_recall, answer_quality_score)
        """
        retrieved_lower = retrieved_text.lower()

        # Precision: must_include í‚¤ì›Œë“œ í¬í•¨ë¥ 
        if must_include:
            included = sum(1 for kw in must_include if kw.lower() in retrieved_lower)
            precision = included / len(must_include)
        else:
            precision = 1.0  # must_includeê°€ ì—†ìœ¼ë©´ ë§Œì 

        # Recall: relevant_keywords ë§¤ì¹­ë¥ 
        if relevant_keywords:
            matched = sum(
                1 for kw in relevant_keywords if kw.lower() in retrieved_lower
            )
            recall = matched / len(relevant_keywords)
        else:
            recall = 1.0

        # ì¢…í•© ì ìˆ˜ (F1-like)
        if precision + recall > 0:
            quality = 2 * precision * recall / (precision + recall)
        else:
            quality = 0.0

        return precision, recall, quality

    def evaluate_single_combination(
        self,
        chunks: List[Dict],
        eval_data: List[Dict],
        embedding_model: str,
        chunking_method: str,
    ) -> EvaluationResult:
        """ë‹¨ì¼ ì¡°í•© í‰ê°€"""

        if not chunks:
            return EvaluationResult(
                chunking_method=chunking_method,
                embedding_model=embedding_model,
                hit_rate_at_1=0.0,
                hit_rate_at_3=0.0,
                hit_rate_at_5=0.0,
                mrr=0.0,
                avg_latency_ms=0.0,
                total_chunks=0,
                embedding_dim=0,
            )

        # 1. ëª¨ë“  ì²­í¬ ì„ë² ë”© ìƒì„±
        print(f"    ì²­í¬ ì„ë² ë”© ìƒì„± ì¤‘... ({len(chunks)}ê°œ)")
        start_time = time.time()

        chunk_texts = [c["text"] for c in chunks]
        chunk_embeddings = self.embedding_wrapper.encode(chunk_texts, embedding_model)

        embed_time = time.time() - start_time

        # 2. ê° ì§ˆë¬¸ì— ëŒ€í•´ í‰ê°€
        hits_at_1, hits_at_3, hits_at_5 = 0, 0, 0
        mrr_sum = 0.0
        query_times = []

        # ë¶€ë¶„ì ìˆ˜ ëˆ„ì 
        precision_sum, recall_sum, quality_sum = 0.0, 0.0, 0.0
        evaluated_count = 0

        for eval_item in eval_data:
            question = eval_item["question"]
            ground_truth = eval_item["ground_truth"]
            eval_criteria = eval_item.get("evaluation_criteria", {})

            # ì •ë‹µ ì²­í¬ ì°¾ê¸° (ìˆ˜ì •ëœ ë°©ì‹)
            answer = ground_truth.get("answer", "")
            keywords = ground_truth.get("relevant_keywords", [])
            expected_ids = ground_truth.get("expected_chunk_ids", [])
            relevant_chunk_ids = self.find_relevant_chunks(
                chunks, answer, keywords, expected_ids
            )

            if not relevant_chunk_ids:
                # ì •ë‹µ ì²­í¬ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš° ìŠ¤í‚µ
                continue

            # ì§ˆë¬¸ ì„ë² ë”©
            q_start = time.time()
            query_embedding = self.embedding_wrapper.encode(
                [question], embedding_model
            )[0]

            # ìœ ì‚¬ë„ ê³„ì‚°
            similarities = []
            for i, chunk_emb in enumerate(chunk_embeddings):
                sim = self.cosine_similarity(query_embedding, chunk_emb)
                similarities.append((i, sim, chunks[i]["id"]))

            query_times.append((time.time() - q_start) * 1000)

            # Top-K ì •ë ¬
            similarities.sort(key=lambda x: x[1], reverse=True)

            # Hit@K ë° MRR ê³„ì‚°
            top_retrieved_text = ""
            for rank, (idx, sim, chunk_id) in enumerate(similarities[:5]):
                if rank == 0:
                    top_retrieved_text = chunks[idx].get("text", "")
                if chunk_id in relevant_chunk_ids:
                    if rank < 1:
                        hits_at_1 += 1
                    if rank < 3:
                        hits_at_3 += 1
                    if rank < 5:
                        hits_at_5 += 1
                    mrr_sum += 1.0 / (rank + 1)
                    break

            # ë¶€ë¶„ì ìˆ˜ ê³„ì‚° (Top-1 ì²­í¬ ê¸°ì¤€)
            must_include = eval_criteria.get("must_include", [])
            relevant_kws = ground_truth.get("relevant_keywords", [])
            prec, rec, qual = self.calculate_answer_quality(
                top_retrieved_text, must_include, relevant_kws
            )
            precision_sum += prec
            recall_sum += rec
            quality_sum += qual
            evaluated_count += 1

        num_questions = len(eval_data)

        return EvaluationResult(
            chunking_method=chunking_method,
            embedding_model=embedding_model,
            hit_rate_at_1=hits_at_1 / num_questions if num_questions > 0 else 0,
            hit_rate_at_3=hits_at_3 / num_questions if num_questions > 0 else 0,
            hit_rate_at_5=hits_at_5 / num_questions if num_questions > 0 else 0,
            mrr=mrr_sum / num_questions if num_questions > 0 else 0,
            avg_latency_ms=np.mean(query_times) if query_times else 0,
            total_chunks=len(chunks),
            embedding_dim=self.embedding_wrapper.get_embedding_dim(embedding_model),
            # ë¶€ë¶„ì ìˆ˜
            keyword_precision=(
                precision_sum / evaluated_count if evaluated_count > 0 else 0
            ),
            keyword_recall=recall_sum / evaluated_count if evaluated_count > 0 else 0,
            answer_quality_score=(
                quality_sum / evaluated_count if evaluated_count > 0 else 0
            ),
        )

    def run_full_evaluation(
        self,
        eval_file: str,
        output_file: str = "evaluation_results.json",
        skip_models: List[str] = None,
    ) -> List[EvaluationResult]:
        """20ê°€ì§€ ì¡°í•© ì „ì²´ í‰ê°€ ì‹¤í–‰"""

        print("=" * 70)
        print("RAG ì‹œìŠ¤í…œ í‰ê°€ ì‹œì‘")
        print(f"ì²­í‚¹ ë°©ì‹: {len(self.chunking_methods)}ê°€ì§€")
        print(f"ì„ë² ë”© ëª¨ë¸: {len(self.embedding_models)}ê°€ì§€")
        print(f"ì´ ì¡°í•©: {len(self.chunking_methods) * len(self.embedding_models)}ê°€ì§€")
        print("=" * 70)

        # í‰ê°€ ë°ì´í„° ë¡œë“œ
        print(f"\n[1] í‰ê°€ ë°ì´í„°ì…‹ ë¡œë“œ: {eval_file}")
        eval_data = self.load_evaluation_dataset(eval_file)
        print(f"    ì§ˆë¬¸ ìˆ˜: {len(eval_data)}ê°œ")

        results = []
        skip_models = skip_models or []

        # ì´ ì¡°í•© ìˆ˜ ê³„ì‚° (ìŠ¤í‚µ ëª¨ë¸ ì œì™¸)
        active_models = [m for m in self.embedding_models if m not in skip_models]
        total_combos = len(self.chunking_methods) * len(active_models)
        combo_idx = 0

        for chunk_folder, chunk_name in self.chunking_methods.items():
            print(f"\n[ì²­í‚¹] {chunk_name}")

            # ì²­í¬ ë¡œë“œ
            chunks = self.load_chunks(chunk_folder)
            print(f"  ë¡œë“œëœ ì²­í¬: {len(chunks)}ê°œ")

            for emb_model in active_models:
                combo_idx += 1

                if not chunks:
                    print(
                        f"  [{combo_idx}/{total_combos}] {chunk_name} + {emb_model} - ìŠ¤í‚µ (ì²­í¬ ì—†ìŒ)"
                    )
                    continue

                print(f"\n  [{combo_idx}/{total_combos}] {chunk_name} + {emb_model}")

                try:
                    test_start_time = time.time()  # ì¶”ê°€: ì‹œì‘ ì‹œê°„

                    result = self.evaluate_single_combination(
                        chunks=chunks,
                        eval_data=eval_data,
                        embedding_model=emb_model,
                        chunking_method=chunk_name,
                    )

                    result.total_time_seconds = (
                        time.time() - test_start_time
                    )  # ì¶”ê°€: ì†Œìš”ì‹œê°„ ê¸°ë¡

                    results.append(result)

                    print(f"    Hit@1: {result.hit_rate_at_1:.2%}")
                    print(f"    Hit@5: {result.hit_rate_at_5:.2%}")
                    print(f"    MRR: {result.mrr:.4f}")
                    print(f"    Latency: {result.avg_latency_ms:.1f}ms")
                    print(
                        f"    Total Time: {result.total_time_seconds:.1f}s"
                    )  # ì´ ì†Œìš”ì‹œê°„ ì¶œë ¥

                except Exception as e:
                    print(f"    [ì˜¤ë¥˜] {e}")

        # ê²°ê³¼ ì €ì¥
        self._save_results(results, output_file, eval_file)

        # ìš”ì•½ ì¶œë ¥
        self._print_summary(results)

        return results

    def _save_results(
        self, results: List[EvaluationResult], output_file: str, eval_file: str
    ):
        """ê²°ê³¼ ì €ì¥"""
        output_data = {
            "evaluation_date": datetime.now().isoformat(),
            "eval_dataset": eval_file,
            "total_combinations": len(results),
            "results": [
                {
                    "chunking_method": r.chunking_method,
                    "embedding_model": r.embedding_model,
                    "hit_rate_at_1": r.hit_rate_at_1,
                    "hit_rate_at_3": r.hit_rate_at_3,
                    "hit_rate_at_5": r.hit_rate_at_5,
                    "mrr": r.mrr,
                    "keyword_precision": r.keyword_precision,
                    "keyword_recall": r.keyword_recall,
                    "answer_quality_score": r.answer_quality_score,
                    "avg_latency_ms": r.avg_latency_ms,
                    "total_chunks": r.total_chunks,
                    "embedding_dim": r.embedding_dim,
                    "total_time_seconds": r.total_time_seconds,
                }
                for r in results
            ],
        }

        output_path = os.path.join(self.base_data_dir, output_file)
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        print(f"\nê²°ê³¼ ì €ì¥: {output_path}")

    def _print_summary(self, results: List[EvaluationResult]):
        """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "=" * 70)
        print("í‰ê°€ ê²°ê³¼ ìš”ì•½ (MRR ê¸°ì¤€ ì •ë ¬)")
        print("=" * 70)

        # MRR ê¸°ì¤€ ì •ë ¬
        sorted_results = sorted(results, key=lambda x: x.mrr, reverse=True)

        print(
            f"{'ìˆœìœ„':<4} {'ì²­í‚¹':<22} {'ì„ë² ë”©':<13} {'Hit@1':>7} {'Hit@5':>7} {'MRR':>7} {'í’ˆì§ˆ':>7}"
        )
        print("-" * 75)

        for i, r in enumerate(sorted_results, 1):
            print(
                f"{i:<4} {r.chunking_method:<22} {r.embedding_model:<13} "
                f"{r.hit_rate_at_1:>6.1%} {r.hit_rate_at_5:>6.1%} {r.mrr:>7.4f} {r.answer_quality_score:>6.1%}"
            )

        # ìµœê³  ì„±ëŠ¥ ì¡°í•©
        if sorted_results:
            best = sorted_results[0]
            print("\n" + "=" * 75)
            print(f"ğŸ† ìµœê³  ì„±ëŠ¥ ì¡°í•©: {best.chunking_method} + {best.embedding_model}")
            print(
                f"   Hit@1: {best.hit_rate_at_1:.1%}, Hit@5: {best.hit_rate_at_5:.1%}, MRR: {best.mrr:.4f}"
            )
            print(
                f"   í‚¤ì›Œë“œ ì •ë°€ë„: {best.keyword_precision:.1%}, ì¬í˜„ìœ¨: {best.keyword_recall:.1%}, í’ˆì§ˆ: {best.answer_quality_score:.1%}"
            )


# ============================================
# ë©”ì¸ ì‹¤í–‰
# ============================================

if __name__ == "__main__":
    # í‰ê°€ ë°ì´í„°ì…‹ íŒŒì¼ ê²½ë¡œ
    EVAL_DATASET = "data/evaluation_dataset2.json"

    # ìŠ¤í‚µí•  ëª¨ë¸ (OpenAI API í‚¤ ì—†ìœ¼ë©´ ìŠ¤í‚µ)
    SKIP_MODELS = [
        "BGE-m3-ko",
    ]  # ["openai"] ë¡œ ì„¤ì •í•˜ë©´ OpenAI ìŠ¤í‚µ

    # API ì—°ë™
    dotenv.load_dotenv()

    # í‰ê°€ê¸° ìƒì„± ë° ì‹¤í–‰
    evaluator = RAGEvaluator(base_data_dir="data")

    # í‰ê°€ ë°ì´í„°ì…‹ì´ ìˆëŠ”ì§€ í™•ì¸
    if not os.path.exists(EVAL_DATASET):
        print(f"[ì˜¤ë¥˜] í‰ê°€ ë°ì´í„°ì…‹ì´ ì—†ìŠµë‹ˆë‹¤: {EVAL_DATASET}")
        print("ë¨¼ì € evaluation_dataset.json íŒŒì¼ì„ ìƒì„±í•´ì£¼ì„¸ìš”.")
    else:
        results = evaluator.run_full_evaluation(
            eval_file=EVAL_DATASET,
            output_file="evaluation_results.json",
            skip_models=SKIP_MODELS,
        )
