import streamlit as st
import os
import time
from langchain_chroma import Chroma  
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage, AIMessage

# ==========================================
# 1. í™”ë©´ ê¸°ë³¸ ì„¤ì •
# ==========================================
st.set_page_config(
    page_title="ì…ì°°ë©”ì´íŠ¸ AI",
    page_icon="ğŸ¤–",
    layout="wide"
)

st.title("ğŸ¤– ì…ì°°/ê³µê³  ë¶„ì„ AI: ì…ì°°ë©”ì´íŠ¸")
st.markdown("ê³µê³µ ì…ì°° ê³µê³ ë¬¸(RFP)ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš”!")

# ==========================================
# 2. ì‚¬ì´ë“œë°” (ì„¤ì •)
# ==========================================
with st.sidebar:
    st.header("âš™ï¸ í™˜ê²½ ì„¤ì •")
    
    if "OPENAI_API_KEY" not in os.environ:
        api_key = st.text_input("OpenAI API Key ì…ë ¥", type="password")
        if api_key:
            os.environ["OPENAI_API_KEY"] = api_key
            st.success("API Key ì €ì¥ ì™„ë£Œ!")
    
    st.subheader("ëª¨ë¸ ì„ íƒ")
    # í”„ë¡œì íŠ¸ ê°€ì´ë“œ ê¸°ì¤€ ëª¨ë¸
    model_options = ["gpt-5-mini", "gpt-5-nano", "gpt-5"]
    selected_model = st.selectbox(
        "ì‚¬ìš©í•  ëª¨ë¸", 
        model_options, 
        index=0
    )
    
    # ëŒ€í™” ë‚´ìš© ì´ˆê¸°í™” ë²„íŠ¼
    if st.button("ğŸ—‘ï¸ ëŒ€í™” ë‚´ìš© ì§€ìš°ê¸°"):
        st.session_state.messages = []
        st.rerun()

# ==========================================
# 3. RAG ì²´ì¸ ì„¤ì • (LCEL & Memory ì ìš©)
# ==========================================
@st.cache_resource(show_spinner="AI ë‘ë‡Œ ë¡œë”© ì¤‘...")
def load_rag_chain(model_name):
    # ê²½ë¡œ ì„¤ì •
    CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
    PROJECT_ROOT = os.path.dirname(CURRENT_DIR)
    DB_PATH = os.path.join(PROJECT_ROOT, "data", "chroma_db")
    
    if not os.path.exists(DB_PATH):
        st.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì—†ìŒ: {DB_PATH}")
        return None

    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    
    # DB ë¡œë“œ
    vectorstore = Chroma(
        persist_directory=DB_PATH,
        embedding_function=embeddings,
        collection_name="bid_rfp_collection"
    )
    
    retriever = vectorstore.as_retriever(
        search_type="mmr",
        search_kwargs={"k": 3, "fetch_k": 10}
    )
    
    try:
        llm = ChatOpenAI(model=model_name, temperature=0)
    except Exception as e:
        st.error(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        return None

    # [1] ì§ˆë¬¸ ì¬êµ¬ì„± (Contextualize)
    context_q_system_prompt = """
    ì±„íŒ… ê¸°ë¡ê³¼ ìµœì‹  ì§ˆë¬¸ì´ ì£¼ì–´ì§€ë©´, ì±„íŒ… ê¸°ë¡ ì—†ì´ë„ ì´í•´í•  ìˆ˜ ìˆëŠ” 
    'ë…ë¦½ì ì¸ ì§ˆë¬¸'ìœ¼ë¡œ ì¬êµ¬ì„±í•˜ì„¸ìš”. ë‹µë³€í•˜ì§€ ë§ê³  ì§ˆë¬¸ë§Œ ë°˜í™˜í•˜ì„¸ìš”.
    """
    context_q_prompt = ChatPromptTemplate.from_messages([
        ("system", context_q_system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ])
    
    history_aware_retriever = context_q_prompt | llm | StrOutputParser()

    # [2] ë‹µë³€ ìƒì„± (QA)
    qa_system_prompt = """
    ë‹¹ì‹ ì€ ê³µê³µ ì…ì°°(RFP) ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    [ê²€ìƒ‰ëœ ë¬¸ì„œ]ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
    
    ê·œì¹™:
    1. ë¬¸ì„œì— ìˆëŠ” ì‚¬ì‹¤ë§Œ ë‹µë³€í•˜ê³ , ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  í•˜ì„¸ìš”.
    2. ë‹µë³€ ëì— ì°¸ê³ í•œ [ë¬¸ì„œëª…]ì„ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”. (ë³„ë„ë¡œ í‘œì‹œë©ë‹ˆë‹¤)
    
    [ê²€ìƒ‰ëœ ë¬¸ì„œ]:
    {context}
    """
    qa_prompt = ChatPromptTemplate.from_messages([
        ("system", qa_system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ])
    
    # [3] ì²´ì¸ ì¡°ë¦½ (LCEL)
    def format_docs(docs):
        return "\n\n".join([d.page_content for d in docs])

    def contextualized_question(input: dict):
        if input.get("chat_history"):
            return history_aware_retriever
        else:
            return input["input"]

    setup_and_retrieval = RunnableParallel(
        {
            "context": contextualized_question | retriever,
            "input": lambda x: x["input"],
            "chat_history": lambda x: x["chat_history"],
        }
    )
    
    # ìµœì¢… ì²´ì¸
    rag_chain = setup_and_retrieval.assign(
        answer=RunnablePassthrough.assign(
            context=lambda x: format_docs(x["context"])
        )
        | qa_prompt
        | llm
        | StrOutputParser()
    )
    
    return rag_chain

# ==========================================
# 4. ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
# ==========================================

# ì„¸ì…˜ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []

# ê¸°ì¡´ ëŒ€í™” ì¶œë ¥
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥
if query := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
    
    if "OPENAI_API_KEY" not in os.environ:
        st.warning("API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        st.stop()

    # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
    st.session_state.messages.append({"role": "user", "content": query})
    with st.chat_message("user"):
        st.markdown(query)

    # ë‹µë³€ ìƒì„±
    with st.chat_message("assistant"):
        chain = load_rag_chain(selected_model)
        
        if chain:
            # LangChain í¬ë§·ìœ¼ë¡œ ëŒ€í™” ê¸°ë¡ ë³€í™˜
            history_langchain = []
            for msg in st.session_state.messages[:-1]: # í˜„ì¬ ì§ˆë¬¸ ì œì™¸
                if msg["role"] == "user":
                    history_langchain.append(HumanMessage(content=msg["content"]))
                else:
                    history_langchain.append(AIMessage(content=msg["content"]))

            message_placeholder = st.empty()
            full_response = ""
            source_docs = []

            # ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
            try:
                for chunk in chain.stream({"input": query, "chat_history": history_langchain}):
                    if "answer" in chunk:
                        full_response += chunk["answer"]
                        message_placeholder.markdown(full_response + "â–Œ")
                    
                    if "context" in chunk:
                        source_docs = chunk["context"]

                message_placeholder.markdown(full_response)
                
                # ì¶œì²˜ í‘œì‹œ (Expander ì‚¬ìš©)
                if source_docs:
                    with st.expander("ğŸ“š ì°¸ê³  ë¬¸ì„œ í™•ì¸í•˜ê¸°"):
                        seen = set()
                        for doc in source_docs:
                            source = os.path.basename(doc.metadata.get("source", "Unknown"))
                            page = doc.metadata.get("page", 0)
                            key = f"{source}p{page}"
                            if key not in seen:
                                st.markdown(f"- **{source}** (Page {page+1})")
                                seen.add(key)

                # ëŒ€í™” ê¸°ë¡ ì €ì¥
                st.session_state.messages.append({"role": "assistant", "content": full_response})

            except Exception as e:
                st.error(f"ì—ëŸ¬ ë°œìƒ: {e}")