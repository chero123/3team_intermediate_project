import streamlit as st
import os
import time
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# ==========================================
# 1. í™”ë©´ ê¸°ë³¸ ì„¤ì •
# ==========================================
st.set_page_config(
    page_title="ì…ì°°ë©”ì´íŠ¸ AI",
    page_icon="ğŸ¤–",
    layout="wide"  # ë„“ì€ í™”ë©´ ì‚¬ìš©
)

st.title("ğŸ¤– ì…ì°°/ê³µê³  ë¶„ì„ AI: ì…ì°°ë©”ì´íŠ¸")
st.markdown("ê³µê³µ ì…ì°° ê³µê³ ë¬¸(RFP)ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš”! (ì˜ˆì‚°, ë§ˆê°ì¼, ìê²©ìš”ê±´ ë“±)")

# ==========================================
# 2. ì‚¬ì´ë“œë°” (ì„¤ì • ë©”ë‰´)
# ==========================================
with st.sidebar:
    st.header("âš™ï¸ í™˜ê²½ ì„¤ì •")
    
    # 1) API í‚¤ ì…ë ¥
    if "OPENAI_API_KEY" not in os.environ:
        api_key = st.text_input("OpenAI API Key ì…ë ¥", type="password")
        if api_key:
            os.environ["OPENAI_API_KEY"] = api_key
            st.success("API Key ì €ì¥ ì™„ë£Œ!")
    
    # 2) ëª¨ë¸ ì„ íƒ
    st.subheader("ëª¨ë¸ ì„ íƒ")
    model_options = ["gpt-5-mini", "gpt-5-nano", "gpt-5"]
    selected_model = st.selectbox(
        "ì‚¬ìš©í•  ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”", 
        model_options, 
        index=0, # ê¸°ë³¸ê°’: gpt-5-mini
        help="gpt-5-miniê°€ ê°€ì„±ë¹„ì™€ ì„±ëŠ¥ ê· í˜•ì´ ê°€ì¥ ì¢‹ìŠµë‹ˆë‹¤."
    )
    
    st.markdown("---")
    st.info("""
    **ğŸ’¡ ì‚¬ìš© íŒ:**
    - "ì´ ì‚¬ì—…ì˜ ì˜ˆì‚°ê³¼ ê¸°ê°„ì€?"
    - "ì°¸ê°€ ìê²© ìš”ê±´ì„ ìš”ì•½í•´ì¤˜"
    - "ì œì•ˆì„œ ì‘ì„± ì‹œ ìœ ì˜ì‚¬í•­ì€?"
    """)

# ==========================================
# 3. RAG ì²´ì¸ ë¡œë”© (ìºì‹± ì ìš©)
# ==========================================
@st.cache_resource(show_spinner="AI ë‘ë‡Œë¥¼ ê¹¨ìš°ëŠ” ì¤‘...")
def load_rag_chain(model_name):
    # ê²½ë¡œ ê³„ì‚°
    CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
    PROJECT_ROOT = os.path.dirname(CURRENT_DIR)
    DB_PATH = os.path.join(PROJECT_ROOT, "data", "chroma_db")
    
    # DB ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    if not os.path.exists(DB_PATH):
        st.error(f"ë°ì´í„°ë² ì´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”: {DB_PATH}")
        return None

    # ì„ë² ë”© & DB ë¡œë“œ
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    vectorstore = Chroma(
        persist_directory=DB_PATH,
        embedding_function=embeddings,
        collection_name="bid_rfp_collection"
    )
    
    # Retriever (MMR: ë‹¤ì–‘ì„± í™•ë³´ ê²€ìƒ‰)
    retriever = vectorstore.as_retriever(
        search_type="mmr",
        search_kwargs={"k": 3, "fetch_k": 10}
    )
    
    # LLM ì„¤ì • (ì„ íƒí•œ ëª¨ë¸ ì ìš©)
    try:
        llm = ChatOpenAI(model=model_name, temperature=0)
    except Exception as e:
        st.error(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        return None
    
    # í”„ë¡¬í”„íŠ¸ (ì „ë¬¸ê°€ í˜ë¥´ì†Œë‚˜)
    template = """
    ë‹¹ì‹ ì€ ê³µê³µ ì…ì°°(RFP) ë¶„ì„ ìˆ˜ì„ ì»¨ì„¤í„´íŠ¸ 'ì…ì°°ë©”ì´íŠ¸'ì…ë‹ˆë‹¤.
    ì•„ë˜ [ê²€ìƒ‰ëœ ë¬¸ì„œ] ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
    
    [ì§€ì¹¨]
    1. ë¬¸ì„œì— ìˆëŠ” ì‚¬ì‹¤ì—ë§Œ ê·¼ê±°í•˜ì—¬ ë‹µë³€í•˜ê³ , ì—†ëŠ” ë‚´ìš©ì€ ì§€ì–´ë‚´ì§€ ë§ˆì„¸ìš”.
    2. ì˜ˆì‚°, ê¸°ê°„, ë‚ ì§œ ë“± ìˆ«ìëŠ” ì •í™•í•˜ê²Œ ëª…ì‹œí•˜ì„¸ìš”.
    3. ë‹µë³€ ëì—ëŠ” ë°˜ë“œì‹œ ì°¸ê³ í•œ [ë¬¸ì„œëª…]ì„ ê´„í˜¸ë¡œ í‘œê¸°í•˜ì„¸ìš”.
    
    [ê²€ìƒ‰ëœ ë¬¸ì„œ]:
    {context}
    
    ì§ˆë¬¸: {question}
    ë‹µë³€:
    """
    prompt = ChatPromptTemplate.from_template(template)
    
    # ë¬¸ì„œ í¬ë§·íŒ… (ì¶œì²˜ í¬í•¨)
    def format_docs(docs):
        return "\n\n".join([f"<ì¶œì²˜: {d.metadata.get('source', 'ë¬¸ì„œëª… ë¯¸ìƒ')}>\n{d.page_content}" for d in docs])
    
    # ì²´ì¸ ê²°í•©
    chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    return chain

# ==========================================
# 4. ë©”ì¸ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
# ==========================================

# ì„¸ì…˜ ê¸°ë¡ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []

# ì´ì „ ëŒ€í™” ë‚´ìš© ê·¸ë¦¬ê¸°
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if query := st.chat_input("ê³µê³  ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”..."):
    # API í‚¤ ì—†ìœ¼ë©´ ì¤‘ë‹¨
    if "OPENAI_API_KEY" not in os.environ:
        st.warning("ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ API Keyë¥¼ ë¨¼ì € ì…ë ¥í•´ì£¼ì„¸ìš”.")
        st.stop()

    # 1. ì‚¬ìš©ì ì§ˆë¬¸ í‘œì‹œ
    st.session_state.messages.append({"role": "user", "content": query})
    with st.chat_message("user"):
        st.markdown(query)

    # 2. AI ë‹µë³€ ìƒì„±
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        
        try:
            # ì²´ì¸ ë¡œë“œ (ì„ íƒëœ ëª¨ë¸ ì‚¬ìš©)
            chain = load_rag_chain(selected_model)
            
            if chain:
                # ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ìƒì„±
                response = chain.invoke(query)
                
                # íƒ€ì ì¹˜ëŠ” íš¨ê³¼
                for chunk in response.split(" "):
                    full_response += chunk + " "
                    time.sleep(0.05)
                    message_placeholder.markdown(full_response + "â–Œ")
                
                message_placeholder.markdown(full_response)
                
                # ëŒ€í™” ê¸°ë¡ ì €ì¥
                st.session_state.messages.append({"role": "assistant", "content": full_response})
        
        except Exception as e:
            st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            if "404" in str(e):
                st.warning("íŒíŠ¸: ì„ íƒí•œ ëª¨ë¸ëª…ì´ ì˜¬ë°”ë¥¸ì§€, API Key ê¶Œí•œì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")