import streamlit as st
import os
import time
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.retrievers import BM25Retriever
from langchain_classic.retrievers.ensemble import EnsembleRetriever
from langchain_community.retrievers import BM25Retriever
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableLambda
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage, AIMessage

# ==========================================
# 1. í™”ë©´ ê¸°ë³¸ ì„¤ì •
# ==========================================
st.set_page_config(
    page_title="ì…ì°°ë©”ì´íŠ¸ AI (Hybrid)",
    page_icon="ğŸ¤–",
    layout="wide"
)

st.title("ì…ì°°/ê³µê³  ë¶„ì„ AI: ì…ì°°ë©”ì´íŠ¸ (Hybrid Edition)")
st.markdown("""
- **Dense(ì˜ë¯¸)**: ë¬¸ë§¥ê³¼ ì˜ë¯¸ë¥¼ íŒŒì•…í•˜ì—¬ ê²€ìƒ‰ (Chroma)
- **Sparse(í‚¤ì›Œë“œ)**: ê³µê³  ë²ˆí˜¸, ì˜ˆì‚°, ëª¨ë¸ëª… ë“± ì •í™•í•œ ë§¤ì¹­ ê²€ìƒ‰ (BM25)
""")

# ==========================================
# 2. ì‚¬ì´ë“œë°” (ì„¤ì •)
# ==========================================
with st.sidebar:
    st.header("âš™ï¸ í™˜ê²½ ì„¤ì •")
    
    if "OPENAI_API_KEY" not in os.environ:
        api_key = st.text_input("OpenAI API Key ì…ë ¥", type="password")
        if api_key:
            os.environ["OPENAI_API_KEY"] = api_key
            st.success("API Key ì €ì¥ ì™„ë£Œ!")
    
    st.subheader("ëª¨ë¸ ì„ íƒ")
    model_options = ["gpt-5-mini", "gpt-5-nano", "gpt-5"]
    selected_model = st.selectbox(
        "ì‚¬ìš©í•  ëª¨ë¸", 
        model_options, 
        index=0
    )

    st.subheader("ê²€ìƒ‰ ê°€ì¤‘ì¹˜ ì„¤ì •")
    dense_weight = st.slider("Dense(ì˜ë¯¸) ë¹„ì¤‘", 0.0, 1.0, 0.6, 0.1, help="ë†’ì„ìˆ˜ë¡ ë¬¸ë§¥ ìœ„ì£¼, ë‚®ì„ìˆ˜ë¡ í‚¤ì›Œë“œ ìœ„ì£¼")
    sparse_weight = round(1.0 - dense_weight, 1)
    st.caption(f"Sparse(í‚¤ì›Œë“œ) ë¹„ì¤‘: {sparse_weight}")
    
    if st.button("ğŸ—‘ï¸ ëŒ€í™” ë‚´ìš© ì§€ìš°ê¸°"):
        st.session_state.messages = []
        st.rerun()

# ==========================================
# 3. RAG ì²´ì¸ ì„¤ì • (Hybrid & LCEL Fix)
# ==========================================
@st.cache_resource(show_spinner="Hybrid ê²€ìƒ‰ ì—”ì§„ ê°€ë™ ì¤‘...")
def load_rag_chain(model_name, dense_w, sparse_w):
    # ê²½ë¡œ ì„¤ì •
    CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
    PROJECT_ROOT = os.path.dirname(CURRENT_DIR)
    DB_PATH = os.path.join(PROJECT_ROOT, "data", "chroma_db")
    
    if not os.path.exists(DB_PATH):
        st.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì—†ìŒ: {DB_PATH}")
        return None

    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    
    # 1. Dense Retriever (Chroma)
    vectorstore = Chroma(
        persist_directory=DB_PATH,
        embedding_function=embeddings,
        collection_name="bid_rfp_collection"
    )
    
    dense_retriever = vectorstore.as_retriever(
        search_type="mmr",
        search_kwargs={"k": 5, "fetch_k": 20}
    )
    
    # 2. Sparse Retriever (BM25)
    try:
        raw_docs = vectorstore.get()
        docs = []
        for i in range(len(raw_docs['ids'])):
            if raw_docs['documents'][i]:
                docs.append(Document(
                    page_content=raw_docs['documents'][i],
                    metadata=raw_docs['metadatas'][i] if raw_docs['metadatas'] else {}
                ))
        
        if not docs:
            st.error("DBì— ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
            
        sparse_retriever = BM25Retriever.from_documents(docs)
        sparse_retriever.k = 5
        
    except Exception as e:
        st.error(f"BM25 ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return None

    # 3. Ensemble Retriever (Hybrid)
    ensemble_retriever = EnsembleRetriever(
        retrievers=[dense_retriever, sparse_retriever],
        weights=[dense_w, sparse_w]
    )
    
    try:
        llm = ChatOpenAI(model=model_name, temperature=0)
    except Exception as e:
        st.error(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        return None

    # [í”„ë¡¬í”„íŠ¸ 1] ì§ˆë¬¸ ì¬êµ¬ì„± (ë…ë¦½ì  ì§ˆë¬¸ ìƒì„±)
    context_q_system_prompt = """
    ì±„íŒ… ê¸°ë¡ê³¼ ìµœì‹  ì§ˆë¬¸ì´ ì£¼ì–´ì§€ë©´, ì±„íŒ… ê¸°ë¡ ì—†ì´ë„ ì´í•´í•  ìˆ˜ ìˆëŠ” 
    'ë…ë¦½ì ì¸ ì§ˆë¬¸'ìœ¼ë¡œ ì¬êµ¬ì„±í•˜ì„¸ìš”. ë‹µë³€í•˜ì§€ ë§ê³  ì§ˆë¬¸ë§Œ ë°˜í™˜í•˜ì„¸ìš”.
    """
    context_q_prompt = ChatPromptTemplate.from_messages([
        ("system", context_q_system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ])
    
    # Chain: (Dict) -> (String)
    history_aware_chain = context_q_prompt | llm | StrOutputParser()

    # [í”„ë¡¬í”„íŠ¸ 2] ë‹µë³€ ìƒì„± (QA)
    qa_system_prompt = """
    ë‹¹ì‹ ì€ ê³µê³µ ì…ì°°(RFP) ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    [ê²€ìƒ‰ëœ ë¬¸ì„œ]ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
    
    ê·œì¹™:
    1. ë¬¸ì„œì— ìˆëŠ” ì‚¬ì‹¤ë§Œ ë‹µë³€í•˜ê³ , ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  í•˜ì„¸ìš”.
    2. ë‹µë³€ ëì— ì°¸ê³ í•œ [ë¬¸ì„œëª…]ì„ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”. (ë³„ë„ë¡œ í‘œì‹œë©ë‹ˆë‹¤)
    
    [ê²€ìƒ‰ëœ ë¬¸ì„œ]:
    {context}
    """
    qa_prompt = ChatPromptTemplate.from_messages([
        ("system", qa_system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ])
    
    # ê²€ìƒ‰ ì¿¼ë¦¬ ê²°ì • í•¨ìˆ˜
    def get_search_query(input_dict):
        if input_dict.get("chat_history"):
            # ëŒ€í™” ê¸°ë¡ì´ ìˆìœ¼ë©´ ì¬êµ¬ì„± ì²´ì¸ ì‹¤í–‰ (String ë°˜í™˜)
            return history_aware_chain.invoke(input_dict)
        else:
            # ì—†ìœ¼ë©´ ì‚¬ìš©ì ì…ë ¥ ê·¸ëŒ€ë¡œ ì‚¬ìš© (String ë°˜í™˜)
            return input_dict["input"]

    # ì²´ì¸ ì¡°ë¦½
    def format_docs(docs):
        return "\n\n".join([d.page_content for d in docs])

    setup_and_retrieval = RunnableParallel(
        {
            # RunnableLambdaë¡œ ê°ì‹¸ì„œ ë¬¸ìì—´ì„ ì•ˆì „í•˜ê²Œ ì „ë‹¬
            "context": RunnableLambda(get_search_query) | ensemble_retriever,
            "input": lambda x: x["input"],
            "chat_history": lambda x: x["chat_history"],
        }
    )
    
    # ìµœì¢… ì²´ì¸
    rag_chain = setup_and_retrieval.assign(
        answer=RunnablePassthrough.assign(
            context=lambda x: format_docs(x["context"])
        )
        | qa_prompt
        | llm
        | StrOutputParser()
    )
    
    return rag_chain

# ==========================================
# 4. ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
# ==========================================

if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if query := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
    
    if "OPENAI_API_KEY" not in os.environ:
        st.warning("API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        st.stop()

    st.session_state.messages.append({"role": "user", "content": query})
    with st.chat_message("user"):
        st.markdown(query)

    with st.chat_message("assistant"):
        chain = load_rag_chain(selected_model, dense_weight, sparse_weight)
        
        if chain:
            history_langchain = []
            for msg in st.session_state.messages[:-1]:
                if msg["role"] == "user":
                    history_langchain.append(HumanMessage(content=msg["content"]))
                else:
                    history_langchain.append(AIMessage(content=msg["content"]))

            message_placeholder = st.empty()
            full_response = ""
            source_docs = []

            try:
                for chunk in chain.stream({"input": query, "chat_history": history_langchain}):
                    if "answer" in chunk:
                        full_response += chunk["answer"]
                        message_placeholder.markdown(full_response + "â–Œ")
                    
                    if "context" in chunk:
                        source_docs = chunk["context"]

                message_placeholder.markdown(full_response)
                
                if source_docs:
                    with st.expander("ğŸ“š ì°¸ê³  ë¬¸ì„œ í™•ì¸í•˜ê¸° (Hybrid ê²€ìƒ‰)"):
                        seen = set()
                        for i, doc in enumerate(source_docs):
                            source = os.path.basename(doc.metadata.get("source", "Unknown"))
                            page = doc.metadata.get("page", 0)
                            preview = doc.page_content[:40].replace("\n", " ")
                            
                            key = f"{source}p{page}"
                            if key not in seen:
                                st.markdown(f"**{i+1}. {source}** (Page {page+1})")
                                st.caption(f"ë‚´ìš©: {preview}...")
                                seen.add(key)

                st.session_state.messages.append({"role": "assistant", "content": full_response})

            except Exception as e:
                st.error(f"ì—ëŸ¬ ë°œìƒ: {e}")